#!/usr/bin/env python3
"""
End-to-end pipeline for TaskFlow AI system.
"""

from .reviewer import review_repository
from .generator import generate_next_task
from .mini_lm import MiniLM
from .rewards import RewardSystem

def pipeline(repo_url, dev_id, skill, last_task):
    """
    Complete pipeline: reviewer â†’ generator â†’ LM â†’ reward â†’ next task
    """
    print("ðŸš€ Starting TaskFlow AI Pipeline")
    print(f"ðŸ“‚ Repository: {repo_url}")
    print(f"ðŸ‘¤ Developer: {dev_id} ({skill})")
    print(f"âœ… Last Task: {last_task}")
    print("-" * 50)

    # Step 1: Review the repository
    print("ðŸ“‹ Step 1: Reviewing repository...")
    review_output = review_repository(repo_url)
    print(f"âœ… Review complete - Score: {review_output['score']}/10")
    print(f"ðŸ’ª Strengths: {len(review_output['strengths'])} items")
    print(f"âš ï¸  Weaknesses: {len(review_output['weaknesses'])} items")
    print()

    # Step 2: Initialize Mini-LM and Reward System
    print("ðŸ¤– Step 2: Initializing AI components...")
    mini_lm = MiniLM()
    reward_system = RewardSystem(mini_lm)
    print("âœ… Mini-LM and Reward System ready")
    print()

    # Step 3: Generate next task using Mini-LM with SCHEMA ENFORCEMENT
    print("ðŸŽ¯ Step 3: Generating next task with Mini-LM + schema enforcement...")

    # Create prompt for Mini-LM task generation
    task_prompt = f"""Generate the next development task for developer '{dev_id}' with skill level '{skill}'.

Last completed task: {last_task}
Repository review score: {review_output['score']}/10
Review feedback: {', '.join(review_output.get('weaknesses', ['various issues']))}

Generate a task in this exact JSON format:
{{
  "task_id": "unique_task_id",
  "title": "Task Title",
  "description": "Detailed description",
  "steps": ["Step 1", "Step 2", "Step 3", "Step 4"],
  "acceptance_criteria": ["Criteria 1", "Criteria 2", "Criteria 3", "Criteria 4"],
  "difficulty": "{skill}",
  "estimated_time": "X-X hours"
}}

Output ONLY valid JSON, no other text."""

    # Load task generation schema
    task_schema_path = os.path.join(os.path.dirname(__file__), 'schemas', 'generator_schema.json')
    with open(task_schema_path) as f:
        task_schema = json.load(f)

    # Generate task with schema enforcement and retry
    max_retries = 3
    task_output = None

    for attempt in range(max_retries):
        try:
            print(f"ðŸ¤– Mini-LM generation attempt {attempt + 1}...")
            raw_output = mini_lm.generate(task_prompt, max_tokens=300, schema=task_schema)
            print(f"âœ… Raw output: {raw_output[:150]}...")

            # Parse and validate
            parsed_task = json.loads(raw_output)
            # Basic validation
            required_fields = ['task_id', 'title', 'description', 'steps', 'acceptance_criteria', 'difficulty', 'estimated_time']
            if all(field in parsed_task for field in required_fields):
                task_output = parsed_task
                print("âœ… Valid task generated by Mini-LM")
                break
            else:
                print(f"âš ï¸  Missing required fields, retrying...")

        except Exception as e:
            print(f"âŒ Generation failed: {e}, retrying...")

    # Fallback to template if Mini-LM fails
    if task_output is None:
        print("âš ï¸  Mini-LM generation failed, using template fallback...")
        task_output = generate_next_task(dev_id, skill, last_task, review_output)

    print(f"ðŸ“‹ Final task: {task_output.get('title', 'Unknown')}")
    print(f"ðŸŽ¯ Description: {task_output.get('description', 'No description')[:100]}...")
    print(f"ðŸ“š Difficulty: {task_output.get('difficulty', 'unknown')}")

    print(f"ðŸ“š Final Task - Difficulty: {task_output.get('difficulty', 'unknown')}")
    print(f"ðŸŽ¯ Next Task: {task_output.get('next_task', task_output.get('task_description', 'Unknown task'))}")
    print()

    # Step 4: Evaluate task generation with RL rewards
    print("ðŸ† Step 4: Evaluating with RL rewards...")
    task_json = json.dumps(task_output)

    reward = reward_system.evaluate_output(task_prompt, task_json, {})
    print(f"ðŸŽ–ï¸  Reward: {reward} (+1 for valid, -1 for invalid)")
    print()

    # Step 5: Run RL Training Loop (Output â†’ Reward â†’ Model Update)
    print("ðŸ§  Step 5: Running RL Training Loop (Output â†’ Reward â†’ Model Update)...")
    try:
        rl_stats = reward_system.run_rl_training_loop(mini_lm, reward_system, num_iterations=3)
        print(f"âœ… RL training completed - {rl_stats['model_updates']} model updates performed")
        print(f"ðŸ“Š Average reward: {sum(rl_stats['avg_rewards'])/len(rl_stats['avg_rewards']):.2f}")
    except Exception as e:
        print(f"âš ï¸  RL training failed: {e}")
    print()

    # Step 6: Final RL stats
    print("ðŸ“Š Step 6: Final RL Performance Stats...")
    rl_stats = reward_system.get_rl_stats()
    print(f"ðŸ§  Final Learning Rate: {rl_stats['learning_rate']:.4f}")
    print(f"ðŸ“ˆ Total RL Steps: {rl_stats['total_steps']}")
    if rl_stats['total_steps'] > 0:
        print(f"ðŸ“Š Average Reward: {rl_stats['average_reward']:.2f}")
    print()

    result = {
        "review": review_output,
        "next_task": task_output,
        "rl_stats": rl_stats,
        "pipeline_status": "completed"
    }

    print("ðŸŽ‰ Pipeline execution completed successfully!")
    return result

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 5:
        print("Usage: python main.py <repo_url> <dev_id> <skill> <last_task>")
        sys.exit(1)

    repo_url = sys.argv[1]
    dev_id = sys.argv[2]
    skill = sys.argv[3]
    last_task = sys.argv[4]

    result = pipeline(repo_url, dev_id, skill, last_task)
    print("Pipeline Result:")
    print(result)